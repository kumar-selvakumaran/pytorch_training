{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dEAqsXtxzZ5yH-k1RmO9IhuOeV_NJ4H9",
      "authorship_tag": "ABX9TyOZUu7dK0a+SfriP/nFU106",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumar-selvakumaran/pytorch_training/blob/main/NN_toys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqVoMJcz6eTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g-b64V-A5vX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Notes:</h1>\n",
        "<h3>\n",
        "1.   pytorch forum says to alter parameters manually only with torch.no_grad():, but tried doing it without. : <a href=\"https://discuss.pytorch.org/t/how-to-manually-set-the-weights-in-a-two-layer-linear-model/45902\">link</a><br><br> dosent work : <br>error :\n",
        "\n",
        "```\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "\n",
        "<ipython-input-160-fbab85b2caea> in <cell line: 1>()\n",
        "----> 1 modelpart.get_parameter(\"layer.weight\")[0][0][0,1] = 0\n",
        "      2\n",
        "      3 # print_pars(modelpart)\n",
        "\n",
        "RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n",
        "```\n",
        "works only with torch.no_grad():<br>\n",
        "maybe related to doing because its not recommended to do inplace opertaions as read in the documentation<br>\n",
        "2.    when using multiple loss functions, to back propogate on each of them, you can just add each of the component losses to make a final loss, and do a final_loss.backward(); optimizer.step(). and this updates all the parameters according to how they contribute to each of these losses. i.e. as intended.\n",
        "\n",
        "4.   Parameters init done in nanogpt repo [link](https://github.com/karpathy/nanoGPT/blob/master/model.py) will help in convergence.\n",
        "\n",
        "\n",
        "5.   **WHY IS THIS HAPENING** :gradient descent step, not multiplying the loss with the gradient, it is doing w_new = w + (alpha * del_J_wrt_w), instead of w_new = w + (alpha * del_J_wrt_w * **J** )"
      ],
      "metadata": {
        "id": "IEJjtUMzH4Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#helper functions KEEP\n",
        "def print_pars(model):\n",
        "  named_pars = tuple(model.named_parameters())\n",
        "  print(f\"parameters are as follows :\")\n",
        "  for ind, i in enumerate(named_pars):\n",
        "    print(f\"\\nparameter : {ind+1} : \\n\\nname : {i[0]}\\n\\nparameters : {i[1]}\\n\\ngradients : {i[1].grad}\")\n",
        "\n",
        "def init_pars(model):\n",
        "  with torch.no_grad():\n",
        "    for name, parameter in model.named_parameters():\n",
        "      parameter = model.get_parameter(name)\n",
        "      if \"bias\" in name:\n",
        "        parameter *= 0\n",
        "      else:\n",
        "        parameter = parameter ** 0\n",
        "      model.get_parameter(name)[0] = parameter\n"
      ],
      "metadata": {
        "id": "BktlKLAoICAr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "<h3> Experiment 1: understanding the compuation graph of convolutional operator, and an optimization step </h3>\n",
        "\n",
        "<h3>- link to image with computation graph of the convolutional operator :\n",
        "\n",
        "[link](https://drive.google.com/file/d/12YDishmlx_VFfI00QXFR7CuZ4VCSpe0z/view?usp=drive_link)\n",
        "\n"
      ],
      "metadata": {
        "id": "TV0CDHs573-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.ones([1,1,3])\n",
        "\n",
        "model = nn.Conv1d(in_channels = 1,\n",
        "                  out_channels = 1,\n",
        "                  kernel_size = 2)\n",
        "\n",
        "init_pars(model)\n",
        "\n",
        "pred = model(inp)\n",
        "\n",
        "gt = torch.ones([1,1,2,1]) * 10\n",
        "gt.requires_grad = True\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1)\n",
        "\n",
        "model.train()\n",
        "\n",
        "lossval = ownloss(gt.sum(), pred.sum())\n",
        "\n",
        "lossval.backward()\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print_pars(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq07j9IyNEck",
        "outputId": "f4ec477e-fc95-4e8c-ffd8-69a49e527301"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters are as follows :\n",
            "\n",
            "parameter : 1 : \n",
            "\n",
            "name : weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[3., 3.]]], requires_grad=True)\n",
            "\n",
            "gradients : tensor([[[-2., -2.]]])\n",
            "\n",
            "parameter : 2 : \n",
            "\n",
            "name : bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([2.], requires_grad=True)\n",
            "\n",
            "gradients : tensor([-2.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "<h3> Experiment 2: multiple heads, and loss functions </h3>\n",
        "\n",
        "<h3>- link to image with computation graph of multple loss functions :\n",
        "\n",
        "[link](https://drive.google.com/file/d/1CAHbP0r1m_LHgJ8ASeCQb2Axug049RdF/view?usp=drive_link)\n",
        "\n"
      ],
      "metadata": {
        "id": "7Ok60lFceDg0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "0AJhXjjfwzQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5be91df-0d4d-4322-e28b-c64f86926756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[16.]]],\n",
            "\n",
            "\n",
            "        [[[16.]]]], grad_fn=<CatBackward0>)\n",
            "parameters are as follows :\n",
            "\n",
            "parameter : 1 : \n",
            "\n",
            "name : conv1.layer.weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[[17., 17.],\n",
            "          [17., 17.]]]], requires_grad=True)\n",
            "\n",
            "gradients : tensor([[[[-16., -16.],\n",
            "          [-16., -16.]]]])\n",
            "\n",
            "parameter : 2 : \n",
            "\n",
            "name : conv1.layer.bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([16.], requires_grad=True)\n",
            "\n",
            "gradients : tensor([-16.])\n",
            "\n",
            "parameter : 3 : \n",
            "\n",
            "name : leaf1.layer.weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[[5., 5.],\n",
            "          [5., 5.]]]], requires_grad=True)\n",
            "\n",
            "gradients : tensor([[[[-4., -4.],\n",
            "          [-4., -4.]]]])\n",
            "\n",
            "parameter : 4 : \n",
            "\n",
            "name : leaf1.layer.bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([1.], requires_grad=True)\n",
            "\n",
            "gradients : tensor([-1.])\n",
            "\n",
            "parameter : 5 : \n",
            "\n",
            "name : leaf2.layer.weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[[13., 13.],\n",
            "          [13., 13.]]]], requires_grad=True)\n",
            "\n",
            "gradients : tensor([[[[-12., -12.],\n",
            "          [-12., -12.]]]])\n",
            "\n",
            "parameter : 6 : \n",
            "\n",
            "name : leaf2.layer.bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([3.], requires_grad=True)\n",
            "\n",
            "gradients : tensor([-3.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class convlayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convlayer, self).__init__()\n",
        "    self.layer = nn.Conv2d(in_channels = 1,\n",
        "                           out_channels = 1,\n",
        "                           kernel_size = 2)\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "class toynn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    conv1 = convlayer()\n",
        "    self.conv1 = conv1\n",
        "\n",
        "    leaf1 = convlayer()\n",
        "    self.leaf1 = leaf1\n",
        "\n",
        "    leaf2 = convlayer()\n",
        "    self.leaf2 = leaf2\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x1 = self.leaf1(x)\n",
        "    x2 = self.leaf2(x)\n",
        "    ypred = torch.cat((x1, x2), axis = 0)\n",
        "    return ypred\n",
        "\n",
        "\n",
        "\n",
        "x = torch.ones((1,1,3,3))\n",
        "model = toynn()\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.get_parameter(\"leaf1.layer.weight\")[0][0][0,0] = 1\n",
        "  model.get_parameter(\"leaf1.layer.bias\")[0] = 0\n",
        "  model.get_parameter(\"leaf1.layer.weight\")[0][0][1,0] = 1\n",
        "  model.get_parameter(\"leaf1.layer.weight\")[0][0][0,1] = 1\n",
        "  model.get_parameter(\"leaf1.layer.weight\")[0][0][1,1] = 1\n",
        "\n",
        "  model.get_parameter(\"leaf2.layer.weight\")[0][0][0,0] = 1\n",
        "  model.get_parameter(\"leaf2.layer.bias\")[0] = 0\n",
        "  model.get_parameter(\"leaf2.layer.weight\")[0][0][1,0] = 1\n",
        "  model.get_parameter(\"leaf2.layer.weight\")[0][0][0,1] = 1\n",
        "  model.get_parameter(\"leaf2.layer.weight\")[0][0][1,1] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model.get_parameter(\"conv1.layer.weight\")[0][0][0,0] = 1\n",
        "  model.get_parameter(\"conv1.layer.bias\")[0] = 0\n",
        "  model.get_parameter(\"conv1.layer.weight\")[0][0][1,0] = 1\n",
        "  model.get_parameter(\"conv1.layer.weight\")[0][0][0,1] = 1\n",
        "  model.get_parameter(\"conv1.layer.weight\")[0][0][1,1] = 1\n",
        "\n",
        "y = model(x)\n",
        "print(y)\n",
        "\n",
        "def ownloss(gt, pred):\n",
        "  return gt - pred\n",
        "\n",
        "def ownloss2(gt, pred):\n",
        "  return -(gt @ pred)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
        "\n",
        "model.train()\n",
        "\n",
        "gt = torch.tensor([[[17.0]], [[3.0]]])\n",
        "gt.requires_grad = True\n",
        "\n",
        "lossval1 = ownloss(gt[0],y[0])\n",
        "lossval2 = ownloss2(gt[1],y[1])\n",
        "# lossval1.backward(retain_graph = True)\n",
        "# lossval2.backward()\n",
        "lv = lossval1+lossval2\n",
        "lv.backward()\n",
        "optimizer.step()\n",
        "print_pars(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "<h3>Experiment 2 : Feature pyramid networks toy</h1>"
      ],
      "metadata": {
        "id": "G4nyvwLofI7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "class layer_block(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(layer_block, self).__init__()\n",
        "    self.block = nn.Conv2d(in_channels = 1,\n",
        "                           out_channels = 1,\n",
        "                           kernel_size = 2,\n",
        "                           stride = 2)\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.block(x)\n",
        "    self.output = output\n",
        "    print(f\"\\noutput : \\n\\n{output}\\n\")\n",
        "    return output\n",
        "\n",
        "class toyfpn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(toyfpn, self).__init__()\n",
        "\n",
        "    self.topdown1 = layer_block()\n",
        "    self.topdown2 = layer_block()\n",
        "    self.topdown3 = layer_block()\n",
        "\n",
        "  def upsample(self, x, mode = 'bilinear'):\n",
        "    x = upsampled = nn.functional.interpolate(input = x,\n",
        "                                              scale_factor = (2,2),\n",
        "                                              mode = mode\n",
        "                                              )\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    outputs = tuple()\n",
        "    print(f\"\\n(TOP DOWN)\\n\")\n",
        "    print(f\"\\ninput : \\n\\n{x}\\n\")\n",
        "    x = self.topdown1(x)\n",
        "    x = self.topdown2(x)\n",
        "    x = self.topdown3(x)\n",
        "\n",
        "    outputs += tuple([x])\n",
        "\n",
        "    print(f\"\\n(BOTTOM UP) \\n\")\n",
        "    x = self.upsample(x)\n",
        "\n",
        "    print(f\"\\n upsampled data : \\n\\n{x}, \\n\\n cross data : \\n\\n{self.topdown2.output}\\n\")\n",
        "\n",
        "    x += self.topdown2.output\n",
        "\n",
        "    outputs += tuple([x])\n",
        "\n",
        "    x = self.upsample(x)\n",
        "\n",
        "    print(f\"\\n upsampled data : \\n\\n{x}, \\n\\n cross data : \\n\\n{self.topdown1.output}\")\n",
        "\n",
        "    x += self.topdown1.output\n",
        "\n",
        "    outputs += tuple([x])\n",
        "\n",
        "    print(f\"\\n(OUTPUTS) : \\n\\n {outputs}\\n\")\n",
        "    return outputs\n",
        "\n",
        "\n",
        "model = toyfpn()\n",
        "\n",
        "init_pars(model)\n",
        "\n",
        "x = torch.ones((1,1,8,8))\n",
        "\n",
        "pred = model(x)"
      ],
      "metadata": {
        "id": "Blo_k4uHHutp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7918c800-3c2d-48e8-e59a-f23f6b8bc70a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(TOP DOWN)\n",
            "\n",
            "\n",
            "input : \n",
            "\n",
            "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
            "\n",
            "\n",
            "output : \n",
            "\n",
            "tensor([[[[4., 4., 4., 4.],\n",
            "          [4., 4., 4., 4.],\n",
            "          [4., 4., 4., 4.],\n",
            "          [4., 4., 4., 4.]]]], grad_fn=<ConvolutionBackward0>)\n",
            "\n",
            "\n",
            "output : \n",
            "\n",
            "tensor([[[[16., 16.],\n",
            "          [16., 16.]]]], grad_fn=<ConvolutionBackward0>)\n",
            "\n",
            "\n",
            "output : \n",
            "\n",
            "tensor([[[[64.]]]], grad_fn=<ConvolutionBackward0>)\n",
            "\n",
            "\n",
            "(BOTTOM UP) \n",
            "\n",
            "\n",
            " upsampled data : \n",
            "\n",
            "tensor([[[[64., 64.],\n",
            "          [64., 64.]]]], grad_fn=<UpsampleBilinear2DBackward0>), \n",
            "\n",
            " cross data : \n",
            "\n",
            "tensor([[[[16., 16.],\n",
            "          [16., 16.]]]], grad_fn=<ConvolutionBackward0>)\n",
            "\n",
            "\n",
            " upsampled data : \n",
            "\n",
            "tensor([[[[80., 80., 80., 80.],\n",
            "          [80., 80., 80., 80.],\n",
            "          [80., 80., 80., 80.],\n",
            "          [80., 80., 80., 80.]]]], grad_fn=<UpsampleBilinear2DBackward0>), \n",
            "\n",
            " cross data : \n",
            "\n",
            "tensor([[[[4., 4., 4., 4.],\n",
            "          [4., 4., 4., 4.],\n",
            "          [4., 4., 4., 4.],\n",
            "          [4., 4., 4., 4.]]]], grad_fn=<ConvolutionBackward0>)\n",
            "\n",
            "(OUTPUTS) : \n",
            "\n",
            " (tensor([[[[64.]]]], grad_fn=<ConvolutionBackward0>), tensor([[[[80., 80.],\n",
            "          [80., 80.]]]], grad_fn=<AddBackward0>), tensor([[[[84., 84., 84., 84.],\n",
            "          [84., 84., 84., 84.],\n",
            "          [84., 84., 84., 84.],\n",
            "          [84., 84., 84., 84.]]]], grad_fn=<AddBackward0>))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_pars(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvHRXtRNET94",
        "outputId": "8b4354fb-3386-4066-b112-6c39b2f5e889"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameters are as follows :\n",
            "\n",
            "parameter : 1 : \n",
            "\n",
            "name : topdown1.block.weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]]]], requires_grad=True)\n",
            "\n",
            "gradients : None\n",
            "\n",
            "parameter : 2 : \n",
            "\n",
            "name : topdown1.block.bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([-0.], requires_grad=True)\n",
            "\n",
            "gradients : None\n",
            "\n",
            "parameter : 3 : \n",
            "\n",
            "name : topdown2.block.weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]]]], requires_grad=True)\n",
            "\n",
            "gradients : None\n",
            "\n",
            "parameter : 4 : \n",
            "\n",
            "name : topdown2.block.bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([0.], requires_grad=True)\n",
            "\n",
            "gradients : None\n",
            "\n",
            "parameter : 5 : \n",
            "\n",
            "name : topdown3.block.weight\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]]]], requires_grad=True)\n",
            "\n",
            "gradients : None\n",
            "\n",
            "parameter : 6 : \n",
            "\n",
            "name : topdown3.block.bias\n",
            "\n",
            "parameters : Parameter containing:\n",
            "tensor([0.], requires_grad=True)\n",
            "\n",
            "gradients : None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt3.sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBgkacvbBtgE",
        "outputId": "0283b829-28ad-4753-b4ab-98298a912d38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(480., grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}